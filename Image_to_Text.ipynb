{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image-to-Text\n",
        "\n",
        "O objetivo deste notebook é construir e treinar um modelo de inteligência artificial capaz de gerar descrições textuais (legendas) para imagens de forma automática. Para isso, utilizaremos o dataset **COCO (Common Objects in Context)** disponível em https://cocodataset.org/#download."
      ],
      "metadata": {
        "id": "aoXfOhbp8yWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalação de dependências"
      ],
      "metadata": {
        "id": "4i99eciQ87q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install pycocotools\n",
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"Dependências instaladas com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t7c5zmN8_Lt",
        "outputId": "de9b891f-4507-4415-8b21-ea7a956c987d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (2.0.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Dependências instaladas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importações"
      ],
      "metadata": {
        "id": "mYmV1WsV9JY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importações de bibliotecas padrão e de machine learning.\n",
        "import os\n",
        "import spacy\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Ferramenta específica para o dataset COCO.\n",
        "from pycocotools.coco import COCO"
      ],
      "metadata": {
        "id": "LQ375xnZ9MY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurações"
      ],
      "metadata": {
        "id": "Tso4lOSL9r-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações de Caminhos e Dados\n",
        "DATA_ROOT = './COCO'\n",
        "IMAGE_DIR = os.path.join(DATA_ROOT, 'images')\n",
        "TRAIN_IMAGE_DIR = os.path.join(IMAGE_DIR, 'train2014')\n",
        "VAL_IMAGE_DIR = os.path.join(IMAGE_DIR, 'val2014')\n",
        "TEST_IMAGE_DIR = os.path.join(IMAGE_DIR, 'test2014')\n",
        "TRAIN_ANNOTATION_FILE = os.path.join(DATA_ROOT, 'annotations/captions_train2014.json')\n",
        "VAL_ANNOTATION_FILE = os.path.join(DATA_ROOT, 'annotations/captions_val2014.json')\n",
        "CHECKPOINT_PATH = 'best_model_checkpoint_attention.pth'\n",
        "\n",
        "# Flag para limitar o dataset para testes rápidos.\n",
        "# Defina como None para usar o dataset completo.\n",
        "MAX_IMAGES = 50000\n",
        "\n"
      ],
      "metadata": {
        "id": "pvjRZ3dT9yC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparação dos Dados (Classes e Funções)\n",
        "\n",
        "Esses quatro tokens especiais são adicionados manualmente no início, com índices fixos:\n",
        "\n",
        "- PAD: para preenchimento (padding)\n",
        "- SOS: início da sequência (Start of Sequence)\n",
        "- EOS: fim da sequência (End of Sequence)\n",
        "- UNK: token desconhecido (para palavras fora do vocabulário)"
      ],
      "metadata": {
        "id": "aUeTXapn-0Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o tokenizador do Spacy para o idioma inglês.\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "2Rm_IQHO-0mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    \"\"\"Cria o vocabulário para mapear palavras para índices numéricos.\"\"\"\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4 # Começa depois dos tokens especiais\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text]\n",
        "\n",
        "class CocoDataset(Dataset):\n",
        "    \"\"\"Classe para carregar os dados do COCO, adaptada para treino, validação e teste.\"\"\"\n",
        "    def __init__(self, root_dir, transform, annotation_file=None, vocab=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.is_test = annotation_file is None\n",
        "\n",
        "        if self.is_test:\n",
        "            # Modo de teste: lê os nomes dos arquivos de imagem diretamente.\n",
        "            self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpg')]\n",
        "            # Extrai os IDs\n",
        "            self.ids = [int(f.split('_')[-1].split('.')[0]) for f in self.image_files]\n",
        "            self.vocab = vocab # Usa o vocabulário já treinado\n",
        "        else:\n",
        "            # Modo de treino/validação: usa o arquivo de anotações.\n",
        "            self.coco = COCO(annotation_file)\n",
        "            self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "            if MAX_IMAGES is not None:\n",
        "                self.ids = self.ids[:MAX_IMAGES]\n",
        "\n",
        "            # Constrói o vocabulário apenas no dataset de treino\n",
        "            if vocab is None:\n",
        "                self.vocab = Vocabulary(freq_threshold=5)\n",
        "                all_captions = [ann['caption'] for ann in self.coco.anns.values()]\n",
        "                self.vocab.build_vocabulary(all_captions)\n",
        "            else:\n",
        "                self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.is_test:\n",
        "            img_id = self.ids[index]\n",
        "            # Encontra o nome do arquivo correspondente ao ID\n",
        "            img_filename = f\"COCO_test2014_{str(img_id).zfill(12)}.jpg\"\n",
        "            img_path = os.path.join(self.root_dir, img_filename)\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            if self.transform is not None:\n",
        "                image = self.transform(image)\n",
        "            return image, img_id\n",
        "        else:\n",
        "            img_id = self.ids[index]\n",
        "            # Pega na primeira legenda disponível para a imagem\n",
        "            caption = self.coco.loadAnns(self.coco.getAnnIds(imgIds=img_id))[0]['caption']\n",
        "            img_path = self.coco.loadImgs(img_id)[0]['file_name']\n",
        "\n",
        "            image = Image.open(os.path.join(self.root_dir, img_path)).convert(\"RGB\")\n",
        "            if self.transform is not None:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "            numericalized_caption += self.vocab.numericalize(caption)\n",
        "            numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "            return image, torch.tensor(numericalized_caption)\n",
        "\n",
        "class MyCollate:\n",
        "    \"\"\"Junta uma lista de amostras para formar um lote (batch), com preenchimento (padding).\"\"\"\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "\n",
        "        targets = [item[1] for item in batch]\n",
        "        lengths = [len(cap) for cap in targets]\n",
        "\n",
        "        padded_targets = torch.full((len(targets), max(lengths)), self.pad_idx, dtype=torch.long)\n",
        "        for i, cap in enumerate(targets):\n",
        "            end = lengths[i]\n",
        "            padded_targets[i, :end] = cap[:end]\n",
        "\n",
        "        return imgs, padded_targets\n",
        "\n",
        "def get_loader(root_dir, transform, batch_size, shuffle, num_workers, annotation_file=None, vocab=None):\n",
        "    \"\"\"Cria e retorna um DataLoader para o dataset COCO.\"\"\"\n",
        "    dataset = CocoDataset(\n",
        "        root_dir=root_dir,\n",
        "        annotation_file=annotation_file,\n",
        "        transform=transform,\n",
        "        vocab=vocab\n",
        "    )\n",
        "\n",
        "    # Obtém o índice de preenchimento do vocabulário do dataset.\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx)\n",
        "    )\n",
        "\n",
        "    return loader, dataset\n",
        "\n",
        "# Define as transformações a serem aplicadas nas imagens.\n",
        "# As médias e desvios padrão são do ImageNet\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "9G6RN2Rr_bEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arquitetura do Modelo"
      ],
      "metadata": {
        "id": "K2C7Fp0oAgNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hiperparâmetros"
      ],
      "metadata": {
        "id": "-tNK5RMCBy_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperparâmetros de Treino\n",
        "BATCH_SIZE = 128          # Tamanho do lote\n",
        "NUM_WORKERS = 8           # Número de processos para carregar dados\n",
        "LEARNING_RATE = 3e-4      # Taxa de aprendizagem inicial para o otimizador\n",
        "NUM_EPOCHS = 50           # Número máximo de épocas de treino\n",
        "PATIENCE_EARLY_STOP = 10  # Paciência para o Early Stopping\n",
        "PATIENCE_LR_SCHEDULER = 3 # Paciência para o redutor de LR\n",
        "\n",
        "# Hiperparâmetros do Modelo\n",
        "EMBED_SIZE = 256        # Dimensão dos vetores de embedding de palavras\n",
        "ATTENTION_DIM = 256     # Dimensão da camada de atenção\n",
        "ENCODER_DIM = 2048      # Dimensão de saída do encoder\n",
        "DECODER_DIM = 512       # Dimensão da camada oculta da LSTM do decoder"
      ],
      "metadata": {
        "id": "lkYnoNyXB2RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder"
      ],
      "metadata": {
        "id": "z9y6vdjsAou6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, encoded_image_size=7, train_backbone=False):\n",
        "        super().__init__()\n",
        "        from torchvision.models import resnet50, ResNet50_Weights\n",
        "        backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "        self.cnn = nn.Sequential(*list(backbone.children())[:-2])   # até camada conv5_x\n",
        "        self.pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "        self.encoder_dim = 2048\n",
        "        if not train_backbone:\n",
        "            for p in self.cnn.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, x):                             # x: (B, 3, 224, 224)\n",
        "        x = self.pool(self.cnn(x))                    # (B, 2048, 7, 7)\n",
        "        x = x.permute(0, 2, 3, 1).flatten(1, 2)       # (B, 49, 2048)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JWYY24iOAhW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder"
      ],
      "metadata": {
        "id": "QFu4cnn1AuKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        super().__init__()\n",
        "        self.attn_feat = nn.Linear(encoder_dim, attention_dim)\n",
        "        self.attn_hidden = nn.Linear(decoder_dim, attention_dim)\n",
        "        self.attn_score = nn.Linear(attention_dim, 1)\n",
        "\n",
        "    def forward(self, feats, h_t):               # feats: (B, 49, enc_dim)\n",
        "        e = torch.tanh(self.attn_feat(feats) + self.attn_hidden(h_t).unsqueeze(1))\n",
        "        e = self.attn_score(e).squeeze(2)        # (B, 49)\n",
        "        alpha = torch.softmax(e, dim=1)          # (B, 49)\n",
        "        context = (feats * alpha.unsqueeze(2)).sum(dim=1)  # (B, enc_dim)\n",
        "        return context, alpha\n",
        "\n",
        "# %%\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, encoder_dim, decoder_dim,\n",
        "                 vocab_size, attention_dim=256, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm      = nn.LSTMCell(embed_size + encoder_dim, decoder_dim)\n",
        "        self.fc        = nn.Linear(decoder_dim, vocab_size)\n",
        "        self.dropout   = nn.Dropout(dropout)\n",
        "        self.decoder_dim = decoder_dim\n",
        "\n",
        "    def init_state(self, B, device):\n",
        "        return (torch.zeros(B, self.decoder_dim, device=device),\n",
        "                torch.zeros(B, self.decoder_dim, device=device))\n",
        "\n",
        "    def forward(self, feats, caps):              # feats: (B, 49, enc_dim)\n",
        "        B, seq_len = caps.size()\n",
        "        embeddings = self.embedding(caps)        # (B, seq_len, embed)\n",
        "        h, c = self.init_state(B, caps.device)\n",
        "        outputs = torch.zeros(B, seq_len-1, self.fc.out_features, device=caps.device)\n",
        "\n",
        "        for t in range(seq_len-1):\n",
        "            context, _ = self.attention(feats, h)\n",
        "            lstm_in = torch.cat([embeddings[:, t, :], context], dim=1)\n",
        "            h, c = self.lstm(lstm_in, (h, c))\n",
        "            outputs[:, t, :] = self.fc(self.dropout(h))\n",
        "        return outputs\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "        features,\n",
        "        max_len: int = 20,\n",
        "        sos_idx: int = 1,\n",
        "        eos_idx: int = 2,\n",
        "        beam_size: int = 5\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Gera uma legenda usando Beam Search para encontrar a frase com a maior\n",
        "        probabilidade total.\n",
        "        \"\"\"\n",
        "        device = features.device\n",
        "        batch_size = features.size(0)\n",
        "        k = beam_size\n",
        "\n",
        "        # O feixe  armazena as k sequências mais prováveis.\n",
        "        # Cada item é uma tupla: (sequência_de_IDs, log_prob_total, estado_h, estado_c)\n",
        "        h, c = self.init_state(batch_size, device)\n",
        "\n",
        "        initial_input = torch.full((batch_size,), sos_idx, device=device, dtype=torch.long)\n",
        "\n",
        "        sequences = [([sos_idx], 0.0, h, c)]\n",
        "\n",
        "        # Itera até o comprimento máximo da legenda\n",
        "        for _ in range(max_len):\n",
        "            all_candidates = []\n",
        "\n",
        "            # Expansão do feixe: para cada sequência candidata, encontra as k melhores próximas palavras.\n",
        "            for seq, score, h_prev, c_prev in sequences:\n",
        "\n",
        "                if seq[-1] == eos_idx:\n",
        "                    all_candidates.append((seq, score, h_prev, c_prev))\n",
        "                    continue\n",
        "\n",
        "                # A entrada para a LSTM é a última palavra da sequência atual.\n",
        "                inputs = torch.tensor([seq[-1]], device=device, dtype=torch.long)\n",
        "                embed = self.embedding(inputs)\n",
        "\n",
        "                context, _ = self.attention(features, h_prev)\n",
        "\n",
        "                lstm_in = torch.cat([embed, context], dim=1)\n",
        "                h_new, c_new = self.lstm(lstm_in, (h_prev, c_prev))\n",
        "\n",
        "                logits = self.fc(h_new)\n",
        "\n",
        "                log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "                # Obtém as k palavras mais prováveis e as suas log-probabilidades\n",
        "                top_log_probs, top_indices = log_probs.topk(k, dim=1)\n",
        "\n",
        "                # Cria k novos candidatos a partir da sequência atual\n",
        "                for i in range(k):\n",
        "                    next_word_idx = top_indices[0, i].item()\n",
        "                    log_p = top_log_probs[0, i].item()\n",
        "\n",
        "                    new_seq = seq + [next_word_idx]\n",
        "                    new_score = score + log_p\n",
        "                    all_candidates.append((new_seq, new_score, h_new, c_new))\n",
        "\n",
        "            ordered_candidates = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # O novo feixe são as k melhores sequências da lista de candidatos.\n",
        "            sequences = ordered_candidates[:k]\n",
        "\n",
        "            if sequences[0][0][-1] == eos_idx:\n",
        "                break\n",
        "\n",
        "        # A melhor sequência final é a primeira da lista (a que tem a maior log-probabilidade)\n",
        "        best_sequence = sequences[0][0]\n",
        "\n",
        "        return best_sequence[1:]\n"
      ],
      "metadata": {
        "id": "yytnXJdJAv4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Completo"
      ],
      "metadata": {
        "id": "QJsrqjXSBgfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, embed_size, encoder_dim, decoder_dim, vocab_size, attention_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderCNN()\n",
        "        self.decoder = DecoderRNN(embed_size, encoder_dim, decoder_dim,\n",
        "                                  vocab_size, attention_dim)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        feats = self.encoder(images)             # (B, 49, 2048)\n",
        "        return self.decoder(feats, captions)"
      ],
      "metadata": {
        "id": "S8aMcRVJE7mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento"
      ],
      "metadata": {
        "id": "yMzhiZCjFWk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo GPU como dispositivo a ser utilizado, se disponível\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")"
      ],
      "metadata": {
        "id": "zpk5CBnYGDAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Carregamento dos Dados"
      ],
      "metadata": {
        "id": "U5vUGyyLFxWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, train_dataset = get_loader(\n",
        "    root_dir=TRAIN_IMAGE_DIR,\n",
        "    annotation_file=TRAIN_ANNOTATION_FILE,\n",
        "    transform=transform,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "print(f\"Dataset de treino carregado com {len(train_dataset)} imagens.\")\n",
        "print(f\"Tamanho do vocabulário construído: {len(train_dataset.vocab)} palavras.\")\n",
        "\n",
        "val_loader, _ = get_loader(\n",
        "    root_dir=VAL_IMAGE_DIR,\n",
        "    annotation_file=VAL_ANNOTATION_FILE,\n",
        "    transform=transform,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    vocab=train_dataset.vocab # Usa o mesmo vocabulário do treino\n",
        ")\n"
      ],
      "metadata": {
        "id": "7CRy-FZbF1de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Funções auxiliares"
      ],
      "metadata": {
        "id": "GrMMLZv5HzME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Para o treino se a perda de validação não melhorar após um determinado número de épocas.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pth'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): Quantas épocas esperar após a última melhoria da perda.\n",
        "            verbose (bool): Se True, imprime uma mensagem para cada melhoria da perda.\n",
        "            delta (float): Mudança mínima para se qualificar como uma melhoria.\n",
        "            path (str): Caminho para guardar o checkpoint do melhor modelo.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} de {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        \"\"\"Guarda o modelo quando a perda de validação diminui.\"\"\"\n",
        "        if self.verbose:\n",
        "            print(f'Perda de validação diminuiu ({self.val_loss_min:.6f} --> {val_loss:.6f}). Guardando modelo ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "uqqpkR1RFY9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(outputs, targets):\n",
        "    \"\"\"Calcula a acurácia por palavra, ignorando o padding.\"\"\"\n",
        "\n",
        "    # Obtém a palavra prevista com a maior probabilidade\n",
        "    _, predicted = outputs.max(2) # outputs.shape: (N, seq_len, vocab_size) -> predicted.shape: (N, seq_len)\n",
        "\n",
        "    # Compara as previsões com os alvos\n",
        "    correct = (predicted == targets)\n",
        "\n",
        "    # Cria uma máscara para ignorar os tokens <PAD> (índice 0)\n",
        "    pad_mask = (targets != 0)\n",
        "\n",
        "    # Aplica a máscara e calcula a acurácia\n",
        "    correct_masked = correct[pad_mask]\n",
        "    num_correct = correct_masked.sum().item()\n",
        "    total_words = pad_mask.sum().item()\n",
        "\n",
        "    accuracy = (num_correct / total_words) * 100 if total_words > 0 else 0\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "RwwDJazGH5lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instanciação do Modelo"
      ],
      "metadata": {
        "id": "KgjBkJ-XGU6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Seq2Seq(\n",
        "    embed_size=EMBED_SIZE,\n",
        "    decoder_dim=DECODER_DIM,\n",
        "    encoder_dim=ENCODER_DIM,\n",
        "    vocab_size=len(train_dataset.vocab),\n",
        "    attention_dim=ATTENTION_DIM,\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    ignore_index=train_dataset.vocab.stoi[\"<PAD>\"],\n",
        "    label_smoothing=0.1\n",
        ")\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=PATIENCE_LR_SCHEDULER)\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    patience=PATIENCE_EARLY_STOP,\n",
        "    verbose=True,\n",
        "    path=CHECKPOINT_PATH\n",
        ")"
      ],
      "metadata": {
        "id": "7P8y3KynGeeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loop de Treino"
      ],
      "metadata": {
        "id": "z-BOWSANJU6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listas para guardar o histórico de treino\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []"
      ],
      "metadata": {
        "id": "PBQ6ev7gtsSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ciclo Principal de Treino\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Época {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    # Fase de Treino\n",
        "    model.train()\n",
        "    train_losses, train_accuracies = [], []\n",
        "    for idx, (imgs, captions) in enumerate(train_loader):\n",
        "        imgs, captions = imgs.to(device), captions.to(device)\n",
        "\n",
        "        targets = captions[:, 1:]\n",
        "\n",
        "        outputs = model(imgs, captions)\n",
        "\n",
        "        loss = criterion(outputs.reshape(-1, outputs.shape[2]),\n",
        "                 targets.reshape(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        accuracy = check_accuracy(outputs, targets)\n",
        "\n",
        "        train_accuracies.append(accuracy)\n",
        "\n",
        "        if (idx + 1) % 100 == 0:\n",
        "            print(f\"  [Treino] Lote {idx+1}/{len(train_loader)}, Perda: {loss.item():.4f}, Acurácia: {accuracy:.2f}%\")\n",
        "\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "    avg_train_acc = sum(train_accuracies) / len(train_accuracies)\n",
        "    train_loss_history.append(avg_train_loss)\n",
        "    train_acc_history.append(avg_train_acc)\n",
        "\n",
        "    # --- Fase de Validação ---\n",
        "    model.eval()\n",
        "    val_losses, val_accuracies = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, captions in val_loader:\n",
        "            imgs, captions = imgs.to(device), captions.to(device)\n",
        "            targets = captions[:, 1:]\n",
        "\n",
        "            outputs  = model(imgs, captions)\n",
        "\n",
        "            loss = criterion(outputs.reshape(-1, outputs.shape[2]),\n",
        "                         targets.reshape(-1))\n",
        "            accuracy = check_accuracy(outputs, targets)\n",
        "            val_losses.append(loss.item())\n",
        "            val_accuracies.append(accuracy)\n",
        "\n",
        "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
        "    avg_val_acc = sum(val_accuracies) / len(val_accuracies)\n",
        "    val_loss_history.append(avg_val_loss)\n",
        "    val_acc_history.append(avg_val_acc)\n",
        "\n",
        "    print(f\"\\nResumo da Época {epoch+1}:\")\n",
        "    print(f\"  Perda de Treino: {avg_train_loss:.4f} | Acurácia de Treino: {avg_train_acc:.2f}%\")\n",
        "    print(f\"  Perda de Validação: {avg_val_loss:.4f} | Acurácia de Validação: {avg_val_acc:.2f}%\\n\")\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "    early_stopping(avg_val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Paragem antecipada ativada!\")\n",
        "        break\n",
        "\n",
        "print(\"\\nTreino Concluído\")"
      ],
      "metadata": {
        "id": "-aw1C9LZJhQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Histórico de Treino"
      ],
      "metadata": {
        "id": "aNcU9toXroi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(train_loss, val_loss, train_acc, val_acc):\n",
        "    \"\"\"Plota os gráficos de perda e acurácia do treino e validação.\"\"\"\n",
        "\n",
        "    # Cria a figura e os eixos para os dois gráficos\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Gráfico da Perda (Loss)\n",
        "    ax1.plot(train_loss, label='Perda de Treino')\n",
        "    ax1.plot(val_loss, label='Perda de Validação')\n",
        "    ax1.set_title('Gráfico de Perda (Loss)', fontsize=16)\n",
        "    ax1.set_xlabel('Épocas', fontsize=12)\n",
        "    ax1.set_ylabel('Perda', fontsize=12)\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Gráfico da Acurácia por Palavra\n",
        "    ax2.plot(train_acc, label='Acurácia de Treino')\n",
        "    ax2.plot(val_acc, label='Acurácia de Validação')\n",
        "    ax2.set_title('Gráfico de Acurácia por Palavra', fontsize=16)\n",
        "    ax2.set_xlabel('Épocas', fontsize=12)\n",
        "    ax2.set_ylabel('Acurácia (%)', fontsize=12)\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Ajusta o layout e exibe os gráficos\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('history.png')\n"
      ],
      "metadata": {
        "id": "-ZCUkwSQr31Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chama a função para plotar os resultados\n",
        "plot_metrics(train_loss_history, val_loss_history, train_acc_history, val_acc_history)"
      ],
      "metadata": {
        "id": "lhIQNApLtnsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testes e Exemplos"
      ],
      "metadata": {
        "id": "hO3NvuoL_A3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption_for_image(model, device, image_path, vocab, transform):\n",
        "    \"\"\"Carrega uma imagem, gera e exibe uma legenda.\"\"\"\n",
        "\n",
        "    # Carrega o melhor modelo\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('best_model_checkpoint_attention.pth', map_location=device))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "    except FileNotFoundError:\n",
        "        print(\"Erro: Ficheiro 'best_model_checkpoint_attention.pth' não encontrado. Treine o modelo primeiro.\")\n",
        "        return\n",
        "\n",
        "    # Prepara a imagem\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    transformed_image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Gera a legenda\n",
        "    with torch.no_grad():\n",
        "        features = model.encoder(transformed_image)\n",
        "        sampled_ids = model.decoder.sample(features)\n",
        "\n",
        "    # Converte os IDs de volta para texto\n",
        "    caption_text = []\n",
        "    for word_id in sampled_ids:\n",
        "        word = vocab.itos[word_id]\n",
        "        if word == \"<EOS>\":\n",
        "            break\n",
        "        if word not in [\"<SOS>\", \"<PAD>\", \"<UNK>\"]:\n",
        "            caption_text.append(word)\n",
        "\n",
        "    # Exibe a imagem e a legenda\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Legenda Gerada: \" + \" \".join(caption_text))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Cria uma pasta para guardar as imagens geradas\n",
        "output_dir = \"generated_captions\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def denormalize(tensor):\n",
        "    \"\"\"Reverte a normalização de um tensor de imagem para exibição.\"\"\"\n",
        "    tensor = tensor.clone()\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406], device=tensor.device)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225], device=tensor.device)\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor\n",
        "\n",
        "def show_and_generate_captions(loader, model, vocab, device, num_examples, title, dataset_name):\n",
        "    \"\"\"Itera sobre um loader, gera legendas e guarda as imagens com as legendas.\"\"\"\n",
        "    print(f\"\\n--- {title} ---\")\n",
        "\n",
        "    try:\n",
        "        # Carrega o modelo salvo\n",
        "        model.load_state_dict(torch.load('best_model_checkpoint_attention.pth', weights_only=True, map_location=device))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "    except FileNotFoundError:\n",
        "        print(\"Erro: Ficheiro 'best_model_checkpoint_attention.pth' não encontrado. Treine o modelo primeiro.\")\n",
        "        return\n",
        "\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for item in loader:\n",
        "            if count >= num_examples:\n",
        "                break\n",
        "\n",
        "            # O loader de teste retorna (img, id), o de validação retorna (img, caption)\n",
        "            img_tensor, original_id = item[0], item[1]\n",
        "            img_tensor = img_tensor.to(device)\n",
        "\n",
        "            features = model.encoder(img_tensor)\n",
        "            sampled_ids = model.decoder.sample(features)\n",
        "\n",
        "            caption_text = []\n",
        "            for word_id in sampled_ids:\n",
        "                word = vocab.itos[word_id]\n",
        "                if word == \"<EOS>\": break\n",
        "                if word not in [\"<SOS>\", \"<PAD>\", \"<UNK>\"]: caption_text.append(word)\n",
        "\n",
        "            final_caption = \" \".join(caption_text)\n",
        "            print(f\"  Imagem {count+1}: {final_caption}\")\n",
        "\n",
        "            img_display = denormalize(img_tensor.cpu().squeeze(0))\n",
        "            img_display = img_display.permute(1, 2, 0).numpy()\n",
        "            img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "            # Salva a imagem com a legenda\n",
        "            plt.figure(figsize=(7, 7))\n",
        "            plt.imshow(img_display)\n",
        "            plt.title(f\"Legenda: {final_caption}\", fontsize=12, wrap=True)\n",
        "            plt.axis(\"off\")\n",
        "            filename = f\"{dataset_name}_image_{count+1}.png\"\n",
        "            plt.savefig(os.path.join(output_dir, filename))\n",
        "            plt.close()\n",
        "\n",
        "            count += 1\n",
        "    print(f\"\\nImagens geradas foram guardadas na pasta '{output_dir}'\")\n",
        "\n",
        "# --- Preparação dos Loaders para Geração ---\n",
        "val_loader_gen, _ = get_loader(\n",
        "    root_dir=VAL_IMAGE_DIR,\n",
        "    annotation_file=VAL_ANNOTATION_FILE,\n",
        "    transform=transform,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    vocab=train_dataset.vocab\n",
        ")\n",
        "\n",
        "TEST_IMAGE_DIR = os.path.join(IMAGE_DIR, 'test2014')\n",
        "if os.path.exists(TEST_IMAGE_DIR):\n",
        "    # CORREÇÃO 3: Removido o collate_fn para o loader de teste.\n",
        "    test_dataset_gen = CocoDataset(root_dir=TEST_IMAGE_DIR, transform=transform, vocab=train_dataset.vocab)\n",
        "    test_loader_gen = DataLoader(\n",
        "        dataset=test_dataset_gen,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS\n",
        "    )\n",
        "else:\n",
        "    test_loader_gen = None\n",
        "    print(f\"\\nAviso: Diretório de teste não encontrado. A geração para o conjunto de teste será ignorada.\")\n",
        "\n",
        "\n",
        "# --- Execução da Geração ---\n",
        "show_and_generate_captions(\n",
        "    loader=val_loader_gen, model=model, vocab=train_dataset.vocab, device=device,\n",
        "    num_examples=50, title=\"Gerando Legendas para as 50 Primeiras Imagens de VALIDAÇÃO\",\n",
        "    dataset_name=\"validation\"\n",
        ")\n",
        "\n",
        "if test_loader_gen:\n",
        "    show_and_generate_captions(\n",
        "        loader=test_loader_gen, model=model, vocab=train_dataset.vocab, device=device,\n",
        "        num_examples=100, title=\"Gerando Legendas para as 100 Primeiras Imagens de TESTE (Não Vistas pelo Modelo)\",\n",
        "        dataset_name=\"test\"\n",
        "    )"
      ],
      "metadata": {
        "id": "2GDDFvv1r_6D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}